{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8f5e07aa-1b4d-4414-bc2f-30b7e51d7136",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"OPENAI_API_KEY\"] = \"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\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7eae8599-0b12-4858-93c5-ff7ed7a2cb29",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "----------Fine Tuning done---------\n",
      "\n",
      "----------Fine Tuning done---------\n",
      "\n",
      "----------Fine Tuning done---------\n"
     ]
    }
   ],
   "source": [
    "# --------------------------- Fine Tuning text-davinci-002 model -----------------------\n",
    "import os\n",
    "import openai\n",
    "\n",
    "openai.api_key = os.getenv(\"OPENAI_API_KEY\")\n",
    "pormpt_prefix = \"Find the main topic disucessed in this social post.\\n post:\"\n",
    "prompt_suffix = \"\\nTopic:\"\n",
    "sentences = [\n",
    "    pormpt_prefix+\"The quick brown fox jumps over the.\"+prompt_suffix+\"it is about a fox. sentiment:neutral\",\n",
    "    pormpt_prefix+\"Once upon a time, there was a king\"+prompt_suffix+\"Starting of a story.sentiment:neutral\",\n",
    "    pormpt_prefix+\"In the beginning, God created the heaven and earth\"+prompt_suffix+\"creation.setiment:positive\",\n",
    "]\n",
    "\n",
    "for sentence in sentences:\n",
    "    response = openai.Completion.create(\n",
    "        engine=\"text-davinci-003\",\n",
    "        prompt=sentence,\n",
    "        max_tokens=280,\n",
    "        n=1,\n",
    "        stop=None,\n",
    "        temperature=0.5,\n",
    "    )\n",
    "    message = response[\"choices\"][0][\"text\"]\n",
    "    print(message)\n",
    "    print(\"----------Fine Tuning done---------\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "30be9e3b-4497-45ea-aa2c-1047ba215952",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Comforters for Sleep\n",
      "\n",
      "\n",
      "Earthquake impact in Turkey/Syria\n",
      "\n",
      "\n",
      "Revival of \"Fawlty Towers\"\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import openai\n",
    "# prompt=\"Find the topic from the following social media content.\\n\\nAfter trying dozens of comforters — sleeping with them for hundreds of hours — @Wirecutter has chosen six that we like for their loft, construction, warmth, and availability. We think they’re absolutely dreamy. https://t.co/d3lHbuci0Z\\nMain Topic: Review of Comforters\\n##\\nIt will be years before cities hit by the devastating earthquake in Turkey and Syria bear any resemblance to their old selves, officials say. \\n\\nWhile intact buildings might look fine to the naked eye, their structural integrity might have been damaged. https://t.co/U1nnOdlu0e\\nMain Topic:  Earthquake Damage in Turkey and Syria\",\n",
    "openai.api_key = os.getenv(\"OPENAI_API_KEY\")\n",
    "pormpt_prefix = \"Find the topic within 6 words from the following social media content.\\n\\n\"\n",
    "# prompt_suffix = \"Main Topic: .Sub Topic: \"\n",
    "# \\n##\\n\n",
    "\n",
    "sentences = [\n",
    "    pormpt_prefix+\"After trying dozens of comforters — sleeping with them for hundreds of hours — @Wirecutter has chosen six that we like for their loft, construction, warmth, and availability. We think they’re absolutely dreamy. https://t.co/d3lHbuci0Z\",\n",
    "    pormpt_prefix+\"It will be years before cities hit by the devastating earthquake in Turkey and Syria bear any resemblance to their old selves, officials say. While intact buildings might look fine to the naked eye, their structural integrity might have been damaged. https://t.co/U1nnOdlu0e\",\n",
    "    pormpt_prefix+\"\\\"Fawlty Towers,\\\" the beloved 1970s British sitcom, is being revived with its original star, John Cleese, and his daughter Camilla, Castle Rock Entertainment said Tuesday. https://t.co/ny2x4kvQHi\",\n",
    "]\n",
    "start_sequence = \"jj\"\n",
    "for sentence in sentences:\n",
    "    response = openai.Completion.create(\n",
    "        engine=\"text-davinci-003\",\n",
    "        prompt=sentence,\n",
    "        max_tokens=560,\n",
    "        top_p=1,\n",
    "        n=1,\n",
    "        stop=None,\n",
    "        temperature=1,\n",
    "        frequency_penalty=0,\n",
    "        presence_penalty=0\n",
    "    )\n",
    "    message = response[\"choices\"][0][\"text\"]\n",
    "    print(message)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "032c7c33-41ae-43d7-9fa8-05580ff33c6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import openai\n",
    "import snscrape.modules.twitter as sntwitter\n",
    "import pandas as pd\n",
    "\n",
    "os.environ[\"OPENAI_API_KEY\"] = \"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\"\n",
    "\n",
    "def extract_topic_using_gpt(sentence):\n",
    "        openai.api_key = os.getenv(\"OPENAI_API_KEY\")\n",
    "        pormpt_prefix = \"Find the topic and sentiment with percenrtage from the following social media content.\\n\\n\"\n",
    "        input_sentence = pormpt_prefix + sentence\n",
    "        response = openai.Completion.create(\n",
    "        engine=\"text-davinci-003\",\n",
    "        prompt=input_sentence,\n",
    "        max_tokens=560,\n",
    "        top_p=1,\n",
    "        n=1,\n",
    "        stop=None,\n",
    "        temperature=1,\n",
    "        frequency_penalty=0,\n",
    "        presence_penalty=0\n",
    "        )\n",
    "        message = response[\"choices\"][0][\"text\"]\n",
    "        keyword = \"Topic:\"\n",
    "        start_index = message.find(keyword)\n",
    "        substring = message[start_index:]\n",
    "        # print(substring)\n",
    "        return substring;\n",
    "    \n",
    "def scrape_tweet_data(username):\n",
    "    # Created a list to append all tweet attributes(data)\n",
    "    attributes_container = []\n",
    "    # Using TwitterSearchScraper to scrape data and append tweets to list\n",
    "    print('=> Start executing crowling soical data for user '+ username)\n",
    "    for i,tweet in enumerate(sntwitter.TwitterSearchScraper('from:'+username).get_items()):\n",
    "        if i>100:\n",
    "            break\n",
    "        post =  tweet.rawContent  \n",
    "        # print(post)\n",
    "        topic = extract_topic_using_gpt(post)\n",
    "        # print(topic)\n",
    "\n",
    "        attributes_container.append([tweet.date, tweet.likeCount, tweet.sourceLabel, tweet.rawContent,topic])\n",
    "    no_of_row = len(attributes_container)    \n",
    "    print('=> No of data row collected = '+ str(no_of_row))\n",
    "    # Creating a dataframe from the tweets list above \n",
    "    tweets_df = pd.DataFrame(attributes_container, columns=[\"Created\", \"Likes\", \"Source\", \"Tweet\",\"Topic\"])\n",
    "    save_as = username+\"_tweets.csv\"\n",
    "    tweets_df.to_csv(save_as, index=False)\n",
    "    file_path = os.path.abspath(save_as)\n",
    "    print(\"=> data saved as CSV at lcoation: \" + file_path)\n",
    "    # print(tweets_df)        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0fbf87f1-78b3-4174-b1b3-a31b22b7c8c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# scrape_tweet_data('united')\n",
    "# scrape_tweet_data('elonmusk')\n",
    "# scrape_tweet_data('iamsrk')\n",
    "# scrape_tweet_data('nytimes')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c0db3b1-cbad-4ed0-8a51-9c84edb1d84f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

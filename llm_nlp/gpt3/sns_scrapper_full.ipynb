{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9861767c-6e32-4762-a921-2bb89a336b83",
   "metadata": {},
   "outputs": [],
   "source": [
    "#  ----------------------------------  using python wraper -------------------\n",
    "#  ---------------------------------- Scrape included word -------------------\n",
    "import os\n",
    "import pandas as pd\n",
    "import snscrape.modules.twitter as twitter\n",
    "import json\n",
    "def scrape_tweet_data(username):\n",
    "    \n",
    "    DATASET_COLUMNS=['tweetDate', 'userId','userName', 'tweetId', 'tweetText', 'retweetedTweet', 'quotedTweet', 'hashtag', 'replyCount', 'retweetCount', 'likeCount', 'viewCount']\n",
    "    attributes_container = []\n",
    "    tweet_count = 5\n",
    "    # text_query = \"Shahrukh Khan\"\n",
    "    # since_date = \"2023-01-01\"\n",
    "    # until_date = \"2023-02-15\"\n",
    "    # for i, tweet in enumerate(twitter.TwitterProfileScraper('iamsrk').get_items()):\n",
    "    for i, tweet in enumerate(twitter.TwitterUserScraper(username).get_items()):\n",
    "    # for i, tweet in enumerate(twitter.TwitterSearchScraper(text_query + ' since:'+since_date+' until:'+until_date+' lang:\"en\" ').get_items()):\n",
    "        if(i == tweet_count):\n",
    "            break\n",
    "        print(tweet)\n",
    "        # print(type(tweet.quotedTweet) )\n",
    "        qt = tweet.quotedTweet\n",
    "        # print(qt)\n",
    "        if(qt is not None):\n",
    "            # d_list = json.loads(qt)\n",
    "            # snscrape.modules.twitter.Tweet qtweet = qt\n",
    "            print(qt.id)\n",
    "        # print d_list.get('id')\n",
    "        rt = tweet.retweetedTweet\n",
    "        # print(rt)\n",
    "        attributes_container.append([tweet.date,tweet.user.id,tweet.user.username,str(tweet.id),tweet.rawContent,rt,qt,tweet.hashtags, tweet.replyCount, tweet.retweetCount, tweet.likeCount,tweet.viewCount])\n",
    "    tweets_df = pd.DataFrame(attributes_container, columns=DATASET_COLUMNS)\n",
    "    save_as = str(username)+\"_tweets.csv\"\n",
    "    tweets_df.to_csv(save_as, index=False)\n",
    "    file_path = os.path.abspath(save_as)\n",
    "    print(\"=> data saved as CSV at lcoation: \" + file_path)\n",
    "    return file_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9308cfdd-5914-4bc1-b12a-3a779f78ae3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#  ----------------------------------  using python wraper -------------------\n",
    "#  ---------------------------------- Scrape included word -------------------\n",
    "import os\n",
    "import snscrape.modules.instagram as sninstagram\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import json\n",
    "data = pd.DataFrame(sninstagram.InstagramUserScraper('shahrukh').get_items())\n",
    "data.head()\n",
    "    \n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e41ae5ba-9a5c-4684-819c-2718f4807913",
   "metadata": {},
   "outputs": [],
   "source": [
    "import snscrape.modules.instagram as sninstagram\n",
    "import pandas as pd\n",
    "\n",
    "query='shahrukh' #change name\n",
    "ins_s=[]\n",
    "limit=10\n",
    "for ins in sninstagram.InstagramHashtagScraper(query).get_items():\n",
    "      print(vars(ins))\n",
    "      break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b2c70869-41ff-4ae5-a0e9-ee00ed405304",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import json\n",
    "def get_tweet_by_date(username,since_date,until_date,max_count):\n",
    "    os.system(\"snscrape --jsonl --max-results {} --since {} twitter-search 'from:{} until:{}'> {}--tweets.json\".format(max_count,since_date,username,until_date,username))\n",
    "    # Reads the json generated from the CLI command above and creates a pandas dataframe\n",
    "    tweets_df = pd.read_json('{}-tweets.json'.format(username), lines=True)\n",
    "    DATASET_COLUMNS=['date', 'id', 'rawContent', 'retweetedTweet', 'quotedTweet', 'hashtags', 'replyCount', 'retweetCount', 'likeCount', 'quoteCount', 'viewCount']\n",
    "    new_df = tweets_df[DATASET_COLUMNS].copy()\n",
    "    return new_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "af6cf2c5-0ca6-4bdc-896a-797f9beb19b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "def fix_nonascii(input_str):\n",
    "    # Convert the string to valid Unicode escape sequences\n",
    "    escaped_str = input_str.encode('unicode_escape').decode('utf-8')\n",
    "    # Remove non-ASCII characters\n",
    "    ascii_str = escaped_str.encode('ascii', 'ignore').decode('utf-8')\n",
    "    # Convert the string back to its original form\n",
    "    fixed_str = ascii_str.encode('utf-8').decode('unicode_escape')\n",
    "    return fixed_str"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4a8c0286-409b-471f-aa5b-5fdb3532b834",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Find the topic and sentiment from the following social media content.\n",
      "\n",
      "@Chad_Hurley \n",
      "\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "replaced = re.sub(r'[^\\x00-\\x7f]',r'', \"Find the topic and sentiment from the following social media content.\\n\\n@Chad_Hurley \\ud83d\\udc40\\n\") \n",
    "print(replaced)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f50d7a7a-af7d-42d5-b7c3-8430725e93fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "import string\n",
    "import re\n",
    "from nltk.corpus import stopwords\n",
    "# nltk.download('stopwords')\n",
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "def clean_text(text):\n",
    "    text = str(text).lower()\n",
    "    text = text.replace('@', 'at')\n",
    "    pat = r\"[{}]\".format(string.punctuation) \n",
    "    text = re.sub(pat, ' ', text) \n",
    "    text = re.sub('  +', ' ', text) \n",
    "    text = re.sub('https?://|www\\.', '', text)  \n",
    "    text = re.sub('\\n', '', text)\n",
    "    # text = re.sub('\\w*\\d\\w*', '', text) \n",
    "    # print(text)\n",
    "    # text = text.replace(u'\\ud800', '')\n",
    "    # text = text.replace('\\x80', '?')\n",
    "    # text = text.translate({i: None for i in range(128, 256)})\n",
    "    # fixed_text = fix_nonascii(text)\n",
    "    text = re.sub(r'[^\\x00-\\x7f]',r'',text)\n",
    "    # text = fixed_text\n",
    "    # print(text)\n",
    "    \n",
    "    # remove stopwords\n",
    "    text = ' '.join([word for word in text.split() if word not in stop_words])\n",
    "    if len(text) == 0:\n",
    "        text = \" \"\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b5066d3-126c-4d64-a5e5-af6dae01a377",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import openai\n",
    "openai.organization = \"org-PcoGruCs6IkfEDRsFbzrDtGA\"\n",
    "openai.api_key = os.getenv(\"OPENAI_API_KEY\")\n",
    "# models = openai.Model.list()\n",
    "\n",
    "# for model in models[\"data\"]:\n",
    "#     print(model[\"id\"])\n",
    "#     # print(model)\n",
    "def getModerationOutput(inputText):\n",
    "    response = openai.Moderation.create(\n",
    "      input=inputText,\n",
    "    )\n",
    "    # convert the OpenAIObject into a JSON string\n",
    "    response_json_str = json.dumps(response)\n",
    "    # parse the JSON response into a dictionary\n",
    "    response_dict = json.loads(response_json_str)\n",
    "    # print(response_dict)\n",
    "    # get the categories dictionary from the first result\n",
    "    categories_dict = response_dict['results'][0]['categories']\n",
    "    isNegative = False\n",
    "    for category, value in categories_dict.items():\n",
    "        # print(category, value)\n",
    "        isNegative = value\n",
    "    # print the categories\n",
    "    # print(categories_dict)\n",
    "    # result = 'Positive'\n",
    "    # if isNegative:\n",
    "    #     result = 'Negative'\n",
    "    # return result\n",
    "    return 'Negative' if isNegative else 'Positive'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58d0844a-557f-4f4d-a5c3-571929bdc5fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_topic_using_gpt(sentence):\n",
    "        openai.api_key = os.getenv(\"OPENAI_API_KEY\")\n",
    "        pormpt_prefix = \"Find the topic and sentiment(positive, negative, nutral) from the following social media content.\\n\\n\"\n",
    "        input_sentence = pormpt_prefix + sentence\n",
    "        response = openai.Completion.create(\n",
    "        engine=\"text-davinci-003\",\n",
    "        prompt=input_sentence,\n",
    "        max_tokens=560,\n",
    "        top_p=1,\n",
    "        n=1,\n",
    "        stop=None,\n",
    "        temperature=1,\n",
    "        frequency_penalty=0,\n",
    "        presence_penalty=0\n",
    "        )\n",
    "        message = response[\"choices\"][0][\"text\"]\n",
    "        keyword = \"Topic:\"\n",
    "        start_index = message.find(keyword)\n",
    "        substring = message[start_index:]\n",
    "        # print(substring)\n",
    "        return substring;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7523e71-776d-4103-bb79-5eab1adff537",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from tabulate import tabulate\n",
    "def showTabulatedDf(df):\n",
    "    # set column width to wrap text\n",
    "    pd.set_option('display.max_colwidth', 10)\n",
    "    \n",
    "    # format dataframe as a table with wrapped text\n",
    "    table = tabulate(df, headers='keys', tablefmt='pipe')\n",
    "    \n",
    "    # print the table\n",
    "    print(table)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51e999bd-b66c-41d2-a48c-98c05abe2fac",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import time\n",
    "from datetime import datetime\n",
    "\n",
    "username = \"ElonMusk\"\n",
    "since_date = \"2023-01-01\"\n",
    "until_date = \"2023-05-29\"\n",
    "max_count = 10000\n",
    "\n",
    "\n",
    "# Get current date and time\n",
    "now = datetime.now()\n",
    "\n",
    "# Format current date as string\n",
    "date_string = now.strftime(\"%Y-%m-%d\")\n",
    "\n",
    "print(date_string)  # Output: 2023-04-15\n",
    "\n",
    "\n",
    "# Calculate the delay based on your rate limit\n",
    "# rate_limit_per_minute = 20\n",
    "# delay = 60.0 / rate_limit_per_minute\n",
    "\n",
    " \n",
    "# def consumeModerationApi(x):\n",
    "#     # wait until there are enough tokens in the bucket\n",
    "#     # while not bucket.consume(1):\n",
    "#     #     time.sleep(5)\n",
    "#      # Sleep for the delay\n",
    "#     time.sleep(delay)\n",
    "#     # make the request\n",
    "#     print(f'making getModerationOutput request with data {x}')\n",
    "#     return getModerationOutput(x)\n",
    "\n",
    "# def consumeCompletionApi(x):\n",
    "#     # wait until there are enough tokens in the bucket\n",
    "#     # while not bucket.consume(1):\n",
    "#     #     time.sleep(5)\n",
    "#      # Sleep for the delay\n",
    "#     time.sleep(delay)\n",
    "#     # make the request\n",
    "#     print(f'making extract_topic_using_gpt request with data {x}')\n",
    "#     return extract_topic_using_gpt(x)\n",
    "\n",
    "\n",
    "tweet_df = get_tweet_by_date(username,since_date,until_date,max_count)\n",
    "print(tweet_df.shape)\n",
    "# print('Missing values percentage')\n",
    "# print(tweet_df.isnull().sum() * 100 / len(tweet_df))\n",
    "print('Missing values counts')\n",
    "print(tweet_df.isnull().sum())\n",
    "# remove stop words\n",
    "tweet_df['clean'] = tweet_df['rawContent'].apply(lambda row: clean_text(row))\n",
    "# print(tweet_df.head())\n",
    "tweet_df.to_csv('{}_tweets_{}.csv'.format(username,date_string), index=False)\n",
    "# showTabulatedDf(tweet_df)\n",
    "# tweet_df['sentiment'] = tweet_df['clean'].apply(lambda x: consumeModerationApi(x))\n",
    "# %time tweet_df['sentimentRaw'] = tweet_df['rawContent'].apply(lambda x: consumeModerationApi(x))\n",
    "# get the topic\n",
    "# %time tweet_df['topic'] = tweet_df['rawContent'].apply(lambda x: consumeCompletionApi(x))\n",
    "# tweet_df['topicClean'] = tweet_df['clean'].apply(lambda x: consumeCompletionApi(x))\n",
    "# tweet_df.head()\n",
    "# df_selected = tweet_df.loc[:, ['rawContent','topic']]\n",
    "# df_selected = tweet_df.loc[:, ['rawContent','topic','sentimentRaw']]\n",
    "# df_selected2 = tweet_df.loc[:, ['clean','topicClean','sentiment']]\n",
    "# print(df_selected.head())\n",
    "# df_selected.head()\n",
    "# showTabulatedDf(df_selected)\n",
    "# showTabulatedDf(df_selected2)\n",
    "# tweet_df.to_csv('{}-tweets.csv'.format(username), index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "206c4d91-16cb-4f85-b134-073cb68db4ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import pandas as pd\n",
    "df = pd.read_csv('iamsrk_tweets_2023-04-17.csv')\n",
    "filename = \"requests_to_parallel_process_iamsrk_2023-04-17_sentiment.jsonl\"\n",
    "n_requests = 10000\n",
    "# pormpt_prefix = \"Find the topic and sentiment(positive, negative, nutral) from the following social media content.\\n\\n\"\n",
    "# pormpt_prefix = \"What are the three most relevant action words that describe the main idea, sentiment, and any mentioned name in this tweet?.\\n\"\n",
    "pormpt_prefix = \"What is the single action words that describe the sentiment in this tweet?.If you find nothing answer neutral?.\\n\"\n",
    "# prompt = f\"Tweet: \\\"{}\\\"\\n\\nPrompt: What is the main idea, sentiment and any mention of a name in the tweet?\\n\\nOutput:\\n\\nIdea: \\nSentiment: \\nName: \"\n",
    "# prompt = f\"Twitter post: \\\"{}\\\"\n",
    "# \\nPrompt: Summarize, Evaluate, Identify\\nMain Idea: Summarize\\nSentiment: Evaluate\\nName: Identify\"\n",
    "# input_sentence = pormpt_prefix + sentence\n",
    "# jobs = [{\"model\": \"text-davinci-003\", \"input\": str(x) + \"\\n\"} for x in range(n_requests)]\n",
    "# jobs = df['rawContent'].apply(lambda row: {\"model\": \"text-embedding-ada-002\", \"prompt\": str(pormpt_prefix+row) + \"\\n\"})\n",
    "jobs = df['clean'].apply(lambda row: {\"model\": \"text-davinci-003\", \"prompt\": str(pormpt_prefix+row) + \"\\n\"})\n",
    "# jobs = df['clean'].apply(lambda row: {\"model\": \"text-davinci-003\", \"prompt\": f\"Tweet: \\\"{row}\\\"\\n\\nPrompt: What is the main idea, sentiment and any mention of a name in the tweet?\\n\\nOutput:\\n\\nIdea: \\nSentiment: \\nName: \"})\n",
    "count = 0\n",
    "with open(filename, \"w\") as f:\n",
    "    for job in jobs:\n",
    "        # job = job.replace('\\x80', '?')\n",
    "        count = count + 1\n",
    "        json_string = json.dumps(job)\n",
    "        f.write(json_string + \"\\n\")\n",
    "        if(count==n_requests):\n",
    "            break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a50a306-ba2f-450a-9035-2c8851440bf6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import pandas as pd\n",
    "# ---\n",
    "with open('news_catcher_data_full.json', 'r') as file:\n",
    "    # Load the JSON data from the file\n",
    "    data = json.load(file)\n",
    "    # data = json.loads(json_data)\n",
    "    # Get a list of titles from the articles\n",
    "    titles = []\n",
    "    for article in data['articles']:\n",
    "        titles.append(article['title'])\n",
    "    # Print the list of titles\n",
    "    print(len(titles))\n",
    "\n",
    "filename = \"requests_to_parallel_process_news_full.jsonl\"\n",
    "# n_requests = 10_000\n",
    "pormpt_prefix = \"Find the main topic from the following news headline title.\\n\\n\"\n",
    "with open(filename, \"w\") as f:\n",
    "    for title in titles:\n",
    "        # job = job.replace('\\x80', '?')\n",
    "        job = {\"model\": \"text-davinci-003\", \"prompt\": str(pormpt_prefix+title) + \"\\n\"}\n",
    "        json_string = json.dumps(job)\n",
    "        f.write(json_string + \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03342b89-e827-4c68-8679-c8a47dac6d56",
   "metadata": {},
   "outputs": [],
   "source": [
    "def getCleanText(content):\n",
    "    # content = \"Find the topic and sentiment from the following social media content.\\n\\nlucky teachers professors teach us fun us also educational rockstars https co\\n!\"\n",
    "\n",
    "    start_index = content.find('\\n\\n') + 2 # Find the index of the first occurrence of \"\\n\\n\" and add 2 to get the start index of the desired text\n",
    "    end_index = content.rfind('\\n') # Find the index of the last occurrence of \"\\n\"\n",
    "    extracted_text = \"Couldn't extract text\"\n",
    "    if start_index != -1 and end_index != -1:\n",
    "        extracted_text = content[start_index:end_index] # Extract the text between start_index and end_index\n",
    "        # print(extracted_text)\n",
    "    else:\n",
    "        print(extracted_text)\n",
    "    return extracted_text    \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f832952-b00e-4866-bfef-003199b2dc28",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "def getTopicAndSentiment(content):\n",
    "    \n",
    "    list_of_dicts = content\n",
    "    for d in list_of_dicts:\n",
    "        for k, v in d.items():\n",
    "            if isinstance(v, str):\n",
    "                d[k] = v.replace(\"'\", '\"')\n",
    "    json_str = json.dumps(list_of_dicts)\n",
    "    \n",
    "    json_obj = json.loads(json_str) # Parse the JSON string into a Python object\n",
    "    resultArray = []\n",
    "    if len(json_obj) > 0:\n",
    "        text = json_obj[0]['text'] # Access the value of the 'text' key in the first element of the list\n",
    "        # text = text.strip()\n",
    "        lines = text.split('\\n') # Split the text into lines using '\\n' as the separator\n",
    "        # print(f'json_obj-0-text == {text}')\n",
    "        # print(len(lines))\n",
    "        # print(lines)\n",
    "        topic = 'none'\n",
    "        sentiment = 'undefined'\n",
    "        if len(lines) > 1:\n",
    "            topicLines = lines[1].split(': ')\n",
    "            if len(topicLines) > 1:\n",
    "                topic = topicLines[1]  # Extract the topic from the second line of the text\n",
    "            else:\n",
    "                topic = lines[1]\n",
    "            # topic = lines[1].split(': ')[1] # Extract the topic from the second line of the text\n",
    "        if len(lines) > 2:\n",
    "            sentimentLines = lines[2].split(': ')\n",
    "            if len(sentimentLines) > 1:\n",
    "                sentiment = sentimentLines[1]  # Extract the sentiment from the third line of the text\n",
    "            # sentiment = lines[2].split(': ')[1] # Extract the sentiment from the third line of the text\n",
    "        # print('Topic==', topic)\n",
    "        # print('Sentiment==', sentiment)\n",
    "        resultArray.extend([topic,sentiment])\n",
    "    else:\n",
    "        print('JSON object is empty')\n",
    "    return resultArray"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99e5c237-0151-4941-951a-b8de09884e0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import pandas as pd\n",
    "username = \"elonmusk\"\n",
    "# username = \"iamsrk\"\n",
    "clean_data = []\n",
    "topics = []\n",
    "sentiments =[]\n",
    "with open(f'parallel_process_results_elonmusk_2023-04-15_100_sentiment.jsonl','r') as file:\n",
    "    for line in file.__iter__():\n",
    "        text = line.strip()\n",
    "        raw_json = json.loads(text)\n",
    "        # print(raw_json)\n",
    "        df = pd.DataFrame(raw_json)\n",
    "        # print(df.shape)\n",
    "        # showTabulatedDf(df)\n",
    "#         extract clean data\n",
    "        clean = df['prompt'][0]\n",
    "        clean = getCleanText(clean)\n",
    "        clean_data.append(clean)\n",
    "        # print(clean_data)\n",
    "#         extract topic and sentiment\n",
    "        topic = df['choices'][1]\n",
    "        topicSentiment = getTopicAndSentiment(topic)\n",
    "        topics.append(topicSentiment[0])\n",
    "        sentiments.append(topicSentiment[1])\n",
    "        # print(topics)\n",
    "        # print(sentiments)\n",
    "data = {'clean':clean_data,'topic':topics, 'sentiment':sentiments}\n",
    "df2 = pd.DataFrame(data) \n",
    "# showTabulatedDf(df2)\n",
    "print('clean-topic-sentiment df')\n",
    "print(df2.shape)\n",
    "# jobs = df['clean'].apply(lambda row: {\"model\": \"text-davinci-003\", \"prompt\": str(pormpt_prefix+row) + \"\\n\"})\n",
    "# ra/gpt_model/elonmusk__tweets.csv\n",
    "df1 = pd.read_csv('elonmusk_tweets_2023-04-15.csv')\n",
    "print(df1.shape)\n",
    "# showTabulatedDf(df1)\n",
    "\n",
    "merged_df = pd.merge(df1,df2, left_on='clean',right_on='clean',how='right')\n",
    "print(merged_df.head())\n",
    "# df['clean'] = tweet_df['rawContent'].apply(lambda row: clean_text(row))\n",
    "# merged_df.to_csv('raw_tweet_data/{}-tweets-with-topics-sentiment.csv'.format(username), index=False)\n",
    "unique_merged_df = merged_df.drop_duplicates(subset=['id','clean'])\n",
    "# unique_merged_df.to_csv('raw_tweet_data/{}-tweets-with-topics-sentiment.csv'.format(username), index=False)\n",
    "print('merged-df')\n",
    "print(merged_df.shape)\n",
    "print('unique_merged_df')\n",
    "print(unique_merged_df.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7abe385-31d0-404b-8601-c35c7589b92c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "# !python -m spacy download en_core_web_sm\n",
    "nlp = spacy.load('en_core_web_sm')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19ea0541-d4b1-4695-8120-94024f08e310",
   "metadata": {},
   "outputs": [],
   "source": [
    "# tweet + topic + sentiment + name\n",
    "import json\n",
    "import pandas as pd\n",
    "import spacy\n",
    "import re\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "# username = \"elonmusk\"\n",
    "username = \"iamsrk\"\n",
    "clean_data = []\n",
    "topics = []\n",
    "sentiments =[]\n",
    "names = []\n",
    "count = 0\n",
    "with open(f'parallel_process_results_iamsrk_2023-04-17_p1.jsonl','r') as file:\n",
    "    for line in file.__iter__():\n",
    "        # count = count +1\n",
    "        # if(count == 6):\n",
    "        #     break\n",
    "        text = line.strip()\n",
    "        raw_json = json.loads(text)\n",
    "\n",
    "        df = pd.DataFrame(raw_json)\n",
    "        sentence = df['prompt'][0]\n",
    "        match = re.search(r'\\n(.*?)\\n', sentence)\n",
    "        if match:\n",
    "            extracted_text = match.group(1)\n",
    "        clean = extracted_text\n",
    "        # print(clean)\n",
    "        clean_data.append(clean)\n",
    "#        extract topic and sentiment\n",
    "        topic = df['choices'][1]\n",
    "        topicJson = topic[0]\n",
    "        sentence = topicJson['text']\n",
    "        # print(sentence)\n",
    "        # input pattern\n",
    "        pattern = sentence\n",
    "        # extract words that start with \"at\" or \"At\"\n",
    "        words = re.findall(r\"\\b(?:a|A)t\\w+\", pattern)\n",
    "        name = ''\n",
    "        if len(words) > 0:\n",
    "            name = words[0][2:]\n",
    "            # print(name)\n",
    "        sentence = sentence.replace(\"Sentiment:\", \"\").replace(\"Name:\", \"\")\n",
    "        doc = nlp(sentence)\n",
    "        # print(doc)\n",
    "        # extract nouns, verbs, adjectives and adverbs\n",
    "        # words = [token.text for token in doc if token.pos_ in ['NOUN', 'VERB', 'ADJ', 'ADV']]\n",
    "        words = [token.text for token in doc if not token.is_space and token.is_alpha]\n",
    "        # main_words.append(words)\n",
    "        # print(words)\n",
    "        topics.append(words)\n",
    "        names.append(name)\n",
    "\n",
    "data3 = {'clean':clean_data,'topic':topics, 'names':names}\n",
    "df3 = pd.DataFrame(data3) \n",
    "print('clean-topic-name df')\n",
    "print(df3.shape)\n",
    "print(df3.head())\n",
    "\n",
    "count = 0;\n",
    "clean_data = []\n",
    "sentiments =[]\n",
    "with open(f'parallel_process_results_iamsrk_2023-04-17_sentiment.jsonl','r') as file:\n",
    "    for line in file.__iter__():\n",
    "        # count = count +1\n",
    "        # if(count == 6):\n",
    "        #     break\n",
    "        text = line.strip()\n",
    "        raw_json = json.loads(text)\n",
    "        df = pd.DataFrame(raw_json)\n",
    "        sentence = df['prompt'][0]\n",
    "        # print(sentence)\n",
    "        match = re.search(r'\\n(.*?)\\n', sentence)\n",
    "        if match:\n",
    "            extracted_text = match.group(1)\n",
    "        clean = extracted_text\n",
    "        # print(clean)\n",
    "        clean_data.append(clean)\n",
    "#        extract topic and sentiment\n",
    "        topic = df['choices'][1]\n",
    "        topicJson = topic[0]\n",
    "        sentence = topicJson['text']\n",
    "        # print(sentence)\n",
    "        doc = nlp(sentence)\n",
    "        words = [token.text for token in doc if not token.is_space and token.is_alpha]\n",
    "        sentiments.append(words)\n",
    "        \n",
    "data2 = {'clean':clean_data,'sentiment':sentiments}\n",
    "df2 = pd.DataFrame(data2) \n",
    "print('clean--sentiment df')\n",
    "print(df2.shape)\n",
    "print(df2.head())\n",
    "\n",
    "df1 = pd.read_csv('iamsrk_tweets_2023-04-17.csv')\n",
    "print(df1.shape)\n",
    "print(df1.head())\n",
    "\n",
    "merged_df = pd.merge(df1,df2, left_on='clean',right_on='clean',how='right')\n",
    "print('1st merged-df')\n",
    "print(merged_df.shape)\n",
    "print(merged_df.head())\n",
    "merged_df = pd.merge(merged_df,df3, left_on='clean',right_on='clean',how='right')\n",
    "print('2nd merged-df')\n",
    "print(merged_df.shape)\n",
    "print(merged_df.head())\n",
    "# df['clean'] = tweet_df['rawContent'].apply(lambda row: clean_text(row))\n",
    "# merged_df.to_csv('raw_tweet_data/{}-tweets-with-topics-sentiment.csv'.format(username), index=False)\n",
    "unique_merged_df = merged_df.drop_duplicates(subset=['id','clean'])\n",
    "# unique_merged_df.to_csv('raw_tweet_data/{}-tweets-with-topics-sentiment.csv'.format(username), index=False)\n",
    "print('unique_merged_df')\n",
    "print(unique_merged_df.shape)\n",
    "print(unique_merged_df.head())\n",
    "unique_merged_df.to_csv('raw_tweet_data/{}-tweets-with-topics-sentiment-name-2023-04-15.csv'.format(username), index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5504a9e9-42e2-499d-9970-1315ff0e1b42",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import pandas as pd\n",
    "clean_data = []\n",
    "topics = []\n",
    "with open(f'raw_tweet_data/requests_to_parallel_process_results_news_full.jsonl','r') as file:\n",
    "    for line in file.__iter__():\n",
    "        text = line.strip()\n",
    "        raw_json = json.loads(text)\n",
    "        # print(raw_json)\n",
    "        df = pd.DataFrame(raw_json)\n",
    "        # print(df.shape)\n",
    "        # showTabulatedDf(df)\n",
    "#         extract clean data\n",
    "        clean = df['prompt'][0]\n",
    "        clean = getCleanText(clean)\n",
    "        clean_data.append(clean)\n",
    "        # print(clean_data)\n",
    "#         extract topic and sentiment\n",
    "        topic = df['choices'][1]\n",
    "        topicSentiment = getTopicAndSentiment(topic)\n",
    "        topics.append(topicSentiment[0])\n",
    "        # print(topics)\n",
    "        # print(sentiments)\n",
    "data = {'topic':topics}\n",
    "df = pd.DataFrame(data) \n",
    "# showTabulatedDf(df1)\n",
    "print('topic df')\n",
    "print(df.shape)\n",
    "df.to_csv('raw_tweet_data/news_topics_full.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31bdd816-78f9-4a55-b242-6c7d53f63b4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import asyncio\n",
    "asyncio.run(\n",
    "        process_api_requests_from_file(\n",
    "            requests_filepath=\"requests_to_parallel_process.jsonl\",\n",
    "            save_filepath=\"requests_to_parallel_process_result.jsonl\",\n",
    "            request_url=\"https://api.openai.com/v1/embeddings\",\n",
    "            api_key=os.getenv(\"OPENAI_API_KEY\"),\n",
    "            max_requests_per_minute=float(20),\n",
    "            max_tokens_per_minute=float(150000),\n",
    "            token_encoding_name=\"cl100k_base\",\n",
    "            max_attempts=int(5),\n",
    "            logging_level=int(20),\n",
    "        )\n",
    " )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58b2a64a-542b-4d6c-b19d-c1e7fbb3af84",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tokenizers\n",
    "\n",
    "# Load the BPE tokenizer with a vocabulary size of 50,000\n",
    "tokenizer = tokenizers.ByteLevelBPETokenizer(vocab_file=None, add_prefix_space=True, num_threads=1, \n",
    "                                              min_frequency=2, lowercase=True)\n",
    "tokenizer.train(files, vocab_size=50000)\n",
    "\n",
    "# Get the BPE encoding for a given text\n",
    "text = \"This is a sample text to encode using BPE.\"\n",
    "encoding = tokenizer.encode(text)\n",
    "\n",
    "# Print the encoded tokens\n",
    "print(encoding.tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "16e53f53-40ed-4fac-a495-989deae2b968",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1217, 3)\n",
      "(605, 2)\n",
      "               word       sentiment\n",
      "0          Remember         Believe\n",
      "1           Believe         Believe\n",
      "2   Congratulations         Believe\n",
      "4        Maybelline  Congratulatory\n",
      "5            Credit  Congratulatory\n",
      "6             Watch           Watch\n",
      "7                At           Watch\n",
      "9          Download        Download\n",
      "10       Knightclub        Download\n",
      "11              App        Download\n",
      "(1189, 1)\n",
      "(604, 1)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/lv/44h1yhq55ld6x4th27m94n4c0000gn/T/ipykernel_18249/2324336251.py:20: FutureWarning: The default value of regex will change from True to False in a future version. In addition, single character regular expressions will *not* be treated as literal strings when regex=True.\n",
      "  df2['sentiment'] = df2['sentiment'].str.replace('[', '').str.replace(']', '').str.replace(\"'\",\"\")\n",
      "/var/folders/lv/44h1yhq55ld6x4th27m94n4c0000gn/T/ipykernel_18249/2324336251.py:21: FutureWarning: The default value of regex will change from True to False in a future version. In addition, single character regular expressions will *not* be treated as literal strings when regex=True.\n",
      "  df2['word'] = df2['topic'].str.replace('[', '').str.replace(']', '').str.replace(\"'\",\"\")\n"
     ]
    }
   ],
   "source": [
    "# topic classification\n",
    "# topic / word - sentiment\n",
    "#  ra/gpt_model/raw_tweet_data/iamsrk-tweets-with-topics-sentiment-name-2023-04-15.csv\n",
    "import pandas as pd\n",
    "import ast\n",
    "import re\n",
    "df_original = pd.read_csv('raw_tweet_data/iamsrk-tweets-with-topics-sentiment-name-2023-04-15.csv')\n",
    "# print(df_original.shape)\n",
    "# print(df_original.head())\n",
    "\n",
    "# df = pd.DataFrame({\n",
    "#     'sentiment': [\"['Neutral']\", \"['Neutral']\", \"['Neutral']\"],\n",
    "#     'topic': [\"['Go', 'Wrong', 'AI']\", \"['Success', 'Excitement', 'Guaranteed']\", \"['Take', 'Walk', 'Outrageous']\"]\n",
    "# })\n",
    "# new_df = pd.DataFrame(columns=['sentiment', 'topic'])\n",
    "df =  df_original.loc[:, ['topic', 'sentiment']]\n",
    "\n",
    "# split the values in the topic column and expand them into separate rows\n",
    "df2 = df.assign(topic=df['topic'].str.split(', ')).explode('topic').reset_index(drop=True)\n",
    "\n",
    "# remove the square brackets from the sentiment and topic values\n",
    "df2['sentiment'] = df2['sentiment'].str.replace('[', '').str.replace(']', '').str.replace(\"'\",\"\")\n",
    "df2['word'] = df2['topic'].str.replace('[', '').str.replace(']', '').str.replace(\"'\",\"\")\n",
    "\n",
    "print(df2.shape)\n",
    "# df2 = df2.drop_duplicates()\n",
    "df2 = df2.drop_duplicates(subset=['word'])\n",
    "df2 =  df2.loc[:, ['word', 'sentiment']]\n",
    "print(df2.shape)\n",
    "print(df2.head(10))\n",
    "df2.to_csv('raw_tweet_data/iamsrk-topic-classification-2023-04-15-with-sentiment.csv', index=False)\n",
    "# Print the new data frame\n",
    "# print(new_df)\n",
    "# Print the new DataFrame\n",
    "# print(new_df)\n",
    "\n",
    "# Print the resulting dataframe\n",
    "# print(new_df.head(10))\n",
    "\n",
    "# convert string to list of strings\n",
    "df_original['topic'] = df_original['topic'].apply(lambda x: ast.literal_eval(x))\n",
    "\n",
    "# create a new data frame with a single column for the individual words\n",
    "df_new = pd.DataFrame({'word': [word for row in df_original['topic'] for word in row]})\n",
    "# drop duplicate rows\n",
    "print(df_new.shape)\n",
    "df_new = df_new.drop_duplicates()\n",
    "print(df_new.shape)\n",
    "# print(df_new.head())\n",
    "# df_new.to_csv('raw_tweet_data/elonmusk-topic-classification-2023-04-15.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "40617488-f873-4f9f-bb52-04821c36b9ce",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "post--sentiment DF\n",
      "(100, 2)\n",
      "post--topic DF\n",
      "(99, 2)\n",
      "merged-df\n",
      "(122, 3)\n",
      "                    post         word  sentiment\n",
      "0    https co i3pdwd5gl7        love,   positive\n",
      "1    https co i3pdwd5gl7       happy,   positive\n",
      "2    https co i3pdwd5gl7     awesome.   positive\n",
      "3    https co j1tx5obii2      follow,   positive\n",
      "4    https co j1tx5obii2        like,   positive\n",
      "5    https co j1tx5obii2     retweet.   positive\n",
      "6  9 https co akfkghxhnz        link,   positive\n",
      "7  9 https co akfkghxhnz  followed by   positive\n",
      "8  9 https co akfkghxhnz       tweet.   positive\n",
      "9    https co dnd3he3w3n      follow,  positive.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/lv/44h1yhq55ld6x4th27m94n4c0000gn/T/ipykernel_12054/1400857903.py:32: FutureWarning: The default value of regex will change from True to False in a future version. In addition, single character regular expressions will *not* be treated as literal strings when regex=True.\n",
      "  df2['sentiment'] = df2['sentiment'].str.replace('[', '').str.replace(']', '').str.replace(\"'\",\"\")\n",
      "/var/folders/lv/44h1yhq55ld6x4th27m94n4c0000gn/T/ipykernel_12054/1400857903.py:33: FutureWarning: The default value of regex will change from True to False in a future version. In addition, single character regular expressions will *not* be treated as literal strings when regex=True.\n",
      "  df2['word'] = df2['word'].str.replace('[', '').str.replace(']', '').str.replace(\"'\",\"\")\n"
     ]
    }
   ],
   "source": [
    "# topic classification\n",
    "# topic / word - sentiment -- from merging prompt, output -- prompt -- output\n",
    "#  ra/gpt_model/raw_tweet_data/iamsrk-tweets-with-topics-sentiment-name-2023-04-15.csv\n",
    "import pandas as pd\n",
    "import ast\n",
    "import re\n",
    "def merge_post_word_sentiment(path_csv_sentiment,path_csv_topic,path_merged_post_word_sentiment):\n",
    "    df2 = pd.read_csv(path_csv_sentiment) \n",
    "    print('post--sentiment DF')\n",
    "    print(df2.shape)\n",
    "    # print(df2.head(1))\n",
    "\n",
    "    df1 = pd.read_csv(path_csv_topic)\n",
    "    print('post--topic DF')\n",
    "    print(df1.shape)\n",
    "    # print(df1.head(1))\n",
    "\n",
    "    merged_df = pd.merge(df1,df2, left_on='Prompt',right_on='Prompt',how='right')\n",
    "    print('merged-df')\n",
    "    print(merged_df.shape)\n",
    "    # print(merged_df.head(1))\n",
    "    merged_df.dropna(subset=['Output_x'], inplace=True)\n",
    "    df_final = pd.DataFrame()\n",
    "    df_final['post'] = merged_df['Prompt']\n",
    "    df_final['word'] = merged_df['Output_x'] \n",
    "    df_final['sentiment'] = merged_df['Output_y']\n",
    "    \n",
    "        # split the values in the topic column and expand them into separate rows\n",
    "    df2 = df_final.assign(word=df_final['word'].str.split(', ')).explode('word').reset_index(drop=True)\n",
    "    \n",
    "        # remove the square brackets from the sentiment and topic values\n",
    "    df2['sentiment'] = df2['sentiment'].str.replace('[', '').str.replace(']', '').str.replace(\"'\",\"\")\n",
    "    df2['word'] = df2['word'].str.replace('[', '').str.replace(']', '').str.replace(\"'\",\"\")\n",
    "    \n",
    "    # df2 = df2.drop_duplicates()\n",
    "    # df2 = df2.dropna()\n",
    "    # print('df_final-df')\n",
    "    print(df2.head(10))\n",
    "    \n",
    "    df2.to_csv(path_merged_post_word_sentiment, index=False)\n",
    "    \n",
    "p_csv_sentiment = 'parsed/result_narendramodi_sentiment.csv'\n",
    "p_csv_topic = 'parsed/result_narendramodi_topic.csv'\n",
    "p_csv_post_topic_sentiment = 'parsed/modi_post_topic_sentiment.csv'\n",
    "merge_post_word_sentiment(p_csv_sentiment,p_csv_topic,p_csv_post_topic_sentiment)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7bca0b36-fc77-42b1-9da1-f2b547c615b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import pandas as pd\n",
    "df = pd.read_csv('raw_tweet_data/elonmusk-topic-classification-2023-04-15.csv')\n",
    "filename = \"requests_to_parallel_process_elonmusk_2023-04-17_topic-classification.jsonl\"\n",
    "n_requests = 10000\n",
    "\n",
    "prompt_prefix = \"Classify the following words into one of the following categories: 1. Personal Information 2. Current Events 3. Hobbies and Interests 4. Entertainment 5. Travel 6. Food and Dining 7. Health and Fitness 8. Relationships 9. Work and Career 10. Education 11. Culture and Society 12. Sports and Recreation 13. Personal Growth and Development 14. Fashion and Style 15. Home and Family 16. Technology and Gadgets 17. Art and Creativity 18. Pets and Animals 19. Philosophy and Ethics 20. Nature and Environment Word: \"\n",
    "\n",
    "jobs = df['word'].apply(lambda row: {\"model\": \"text-davinci-003\", \"prompt\": str(prompt_prefix+row) + \"\\n\"})\n",
    "count = 0\n",
    "with open(filename, \"w\") as f:\n",
    "    for job in jobs:\n",
    "        # job = job.replace('\\x80', '?')\n",
    "        count = count + 1\n",
    "        json_string = json.dumps(job)\n",
    "        f.write(json_string + \"\\n\")\n",
    "        if(count==n_requests):\n",
    "            break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "bb4bfa23-cb44-4c63-b0c8-419fd47fed67",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "topic--class df\n",
      "(604, 2)\n",
      "              word                            class\n",
      "0            Watch           Technology and Gadgets\n",
      "1    Inspirational  Personal Growth and Development\n",
      "2               At                   Current Events\n",
      "3          Showing                Fashion and Style\n",
      "4  Congratulations                    Relationships\n",
      "(605, 2)\n",
      "              word       sentiment\n",
      "0         Remember         Believe\n",
      "1          Believe         Believe\n",
      "2  Congratulations         Believe\n",
      "3       Maybelline  Congratulatory\n",
      "4           Credit  Congratulatory\n",
      "1st merged-df\n",
      "(604, 3)\n",
      "              word     sentiment                            class\n",
      "0            Watch         Watch           Technology and Gadgets\n",
      "1    Inspirational  Appreciation  Personal Growth and Development\n",
      "2               At         Watch                   Current Events\n",
      "3          Showing  Appreciation                Fashion and Style\n",
      "4  Congratulations       Believe                    Relationships\n"
     ]
    }
   ],
   "source": [
    "# word sentiment class\n",
    "\n",
    "import json\n",
    "import pandas as pd\n",
    "import spacy\n",
    "import re\n",
    "count = 0;\n",
    "clean_data = []\n",
    "classes =[]\n",
    "with open(f'parallel_process_results_iamsrk_2023-04-17_topic-classification.jsonl','r') as file:\n",
    "    for line in file.__iter__():\n",
    "        errorFound = False\n",
    "        count = count +1\n",
    "        if(count == 10100):\n",
    "            break\n",
    "        text = line.strip()\n",
    "        raw_json = json.loads(text)\n",
    "        # print(count)\n",
    "        # print(raw_json)\n",
    "        for item in raw_json[1]:\n",
    "            if 'error' in item:\n",
    "                # print(item['error']['message'])\n",
    "                errorFound = True\n",
    "        if errorFound:\n",
    "            continue\n",
    "        df = pd.DataFrame(raw_json)\n",
    "        sentence = df['prompt'][0]\n",
    "        # print(sentence)\n",
    "        match = re.search(r'Word: (.*?)\\n', sentence)\n",
    "        if match:\n",
    "            extracted_text = match.group(1)\n",
    "        clean = extracted_text\n",
    "        # print(clean)\n",
    "        clean_data.append(clean)\n",
    "#        extract topic and sentiment\n",
    "        topic = df['choices'][1]\n",
    "        topicJson = topic[0]\n",
    "        sentence = topicJson['text']\n",
    "        # print(sentence)\n",
    "        words = sentence.strip()\n",
    "        classes.append(words)\n",
    "        \n",
    "data2 = {'word':clean_data,'class':classes}\n",
    "df2 = pd.DataFrame(data2) \n",
    "print('topic--class df')\n",
    "print(df2.shape)\n",
    "print(df2.head())\n",
    "\n",
    "df1 = pd.read_csv('raw_tweet_data/iamsrk-topic-classification-2023-04-15-with-sentiment.csv')\n",
    "print(df1.shape)\n",
    "print(df1.head())\n",
    "\n",
    "merged_df = pd.merge(df1,df2, left_on='word',right_on='word',how='right')\n",
    "print('1st merged-df')\n",
    "print(merged_df.shape)\n",
    "print(merged_df.head())\n",
    "merged_df.to_csv('raw_tweet_data/iamsrk-topic-classification-2023-04-15-merged-with-sentiment.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "7cbc7def-06d3-4499-9f5e-8462b7d1fd23",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "df = pd.read_json('raw_tweet_data/result_class_cnn.jsonl', lines=True)\n",
    "df.to_csv('parsed/result_class_cnn.csv', index=False)\n",
    "\n",
    "df = pd.read_json('raw_tweet_data/result_class_justin.jsonl', lines=True)\n",
    "df.to_csv('parsed/result_class_justin.csv', index=False)\n",
    "\n",
    "df = pd.read_json('raw_tweet_data/result_class_kamla.jsonl', lines=True)\n",
    "df.to_csv('parsed/result_class_kamla.csv', index=False)\n",
    "\n",
    "\n",
    "df = pd.read_json('raw_tweet_data/result_class_messi.jsonl', lines=True)\n",
    "df.to_csv('parsed/result_class_messi.csv', index=False)\n",
    "\n",
    "df = pd.read_json('raw_tweet_data/result_class_modi.josnl', lines=True)\n",
    "df.to_csv('parsed/result_class_modi.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "dc72620b-6d46-414c-ba23-a8e5382b6165",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "topic--class df\n",
      "(604, 2)\n",
      "            word                            class\n",
      "0          Watch           Technology and Gadgets\n",
      "1  Inspirational  Personal Growth and Development\n",
      "(605, 2)\n",
      "       word sentiment\n",
      "0  Remember   Believe\n",
      "1   Believe   Believe\n",
      "1st merged-df\n",
      "(604, 3)\n",
      "              word     sentiment                            class\n",
      "0            Watch         Watch           Technology and Gadgets\n",
      "1    Inspirational  Appreciation  Personal Growth and Development\n",
      "2               At         Watch                   Current Events\n",
      "3          Showing  Appreciation                Fashion and Style\n",
      "4  Congratulations       Believe                    Relationships\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import pandas as pd\n",
    "import spacy\n",
    "import re\n",
    "\n",
    "def merge_word_sentiment_class(path_topic_class_jsonl,path_topic_class_sentiment,path_word_sentiment_class):\n",
    "    # word sentiment class\n",
    "    count = 0;\n",
    "    clean_data = []\n",
    "    classes =[]\n",
    "    with open(path_topic_class_jsonl,'r') as file:\n",
    "        for line in file.__iter__():\n",
    "            errorFound = False\n",
    "            count = count +1\n",
    "            if(count == 10100):\n",
    "                break\n",
    "            text = line.strip()\n",
    "            raw_json = json.loads(text)\n",
    "            # print(count)\n",
    "            # print(raw_json)\n",
    "            for item in raw_json[1]:\n",
    "                if 'error' in item:\n",
    "                    # print(item['error']['message'])\n",
    "                    errorFound = True\n",
    "            if errorFound:\n",
    "                continue\n",
    "            df = pd.DataFrame(raw_json)\n",
    "            sentence = df['prompt'][0]\n",
    "            # print(sentence)\n",
    "            match = re.search(r'Word: (.*?)\\n', sentence)\n",
    "            if match:\n",
    "                extracted_text = match.group(1)\n",
    "            clean = extracted_text\n",
    "            # print(clean)\n",
    "            clean_data.append(clean)\n",
    "    #        extract topic and sentiment\n",
    "            topic = df['choices'][1]\n",
    "            topicJson = topic[0]\n",
    "            sentence = topicJson['text']\n",
    "            # print(sentence)\n",
    "            words = sentence.strip()\n",
    "            classes.append(words)\n",
    "\n",
    "    data2 = {'word':clean_data,'class':classes}\n",
    "    df2 = pd.DataFrame(data2) \n",
    "    print('topic--class df')\n",
    "    print(df2.shape)\n",
    "    print(df2.head(2))\n",
    "\n",
    "    df1 = pd.read_csv(path_topic_class_sentiment)\n",
    "    print(df1.shape)\n",
    "    print(df1.head(2))\n",
    "\n",
    "    merged_df = pd.merge(df1,df2, left_on='word',right_on='word',how='right')\n",
    "    print('1st merged-df')\n",
    "    print(merged_df.shape)\n",
    "    print(merged_df.head())\n",
    "    merged_df.to_csv(p_word_sentiment_class, index=False)\n",
    "    \n",
    "p_topic_class = 'parallel_process_results_iamsrk_2023-04-17_topic-classification.jsonl'\n",
    "p_topic_sentiment = 'raw_tweet_data/iamsrk-topic-classification-2023-04-15-with-sentiment.csv'\n",
    "p_word_sentiment_class = 'posts/iamsrk_word_sentiment_class.csv'\n",
    "merge_word_sentiment_class(p_topic_class,p_topic_sentiment,p_word_sentiment_class)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "27784d6e-6a8f-452d-8fb5-65ddef12b933",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(279, 3)\n",
      "                                                post        word sentiment\n",
      "0  sister maya chance speak talented asia pacific...     action,  positive\n",
      "1  sister maya chance speak talented asia pacific...  important,  positive\n",
      "(329, 2)\n",
      "        word            class\n",
      "0     action        Education\n",
      "1  important  Work and Career\n",
      "1st merged-df\n",
      "(935, 4)\n",
      "                                                post        word sentiment  \\\n",
      "0                                                NaN      action       NaN   \n",
      "1                                                NaN   important       NaN   \n",
      "2                                                NaN  leadership       NaN   \n",
      "3  rise antisemitic attacks alarm us antisemitism...        rise     alarm   \n",
      "4  rise antisemitic attacks alarm us antisemitism...       alarm     alarm   \n",
      "\n",
      "                             class  \n",
      "0                        Education  \n",
      "1                  Work and Career  \n",
      "2                  Work and Career  \n",
      "3  Personal Growth and Development  \n",
      "4               Health and Fitness  \n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import pandas as pd\n",
    "import spacy\n",
    "import re\n",
    "\n",
    "def merge_word_sentiment_class_csv(p_psot_topic_sentiment_class,p_word_class,p_to_save):\n",
    "    # word sentiment class\n",
    "\n",
    "\n",
    "    df1 = pd.read_csv(p_psot_topic_sentiment_class)\n",
    "    print(df1.shape)\n",
    "    print(df1.head(2))\n",
    "\n",
    "    df2 = pd.read_csv(p_word_class)\n",
    "    print(df2.shape)\n",
    "    print(df2.head(2))\n",
    "\n",
    "    merged_df = pd.merge(df1,df2, left_on='word',right_on='word',how='right')\n",
    "    print('1st merged-df')\n",
    "    print(merged_df.shape)\n",
    "    print(merged_df.head())\n",
    "    merged_df.to_csv(p_to_save)\n",
    "    \n",
    "p_psot_topic_sentiment_class = 'parsed/barak_post_topic_sentiment.csv'\n",
    "p_word_class = 'parsed/result_class_barak.csv'\n",
    "p_to_save = 'parsed/post_topic_sentiment_class_barak.json'\n",
    "merge_word_sentiment_class_csv(p_psot_topic_sentiment_class,p_word_class,p_to_save)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "655ac271-3bff-4700-9ad3-d54008dd66d6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4164a3d-125f-4634-adc1-77ce3918d17c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "37110c62-a514-4e7a-b8b3-e6ba8dc974f2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 2606 entries, 0 to 2605\n",
      "Data columns (total 15 columns):\n",
      " #   Column          Non-Null Count  Dtype  \n",
      "---  ------          --------------  -----  \n",
      " 0   date            2606 non-null   object \n",
      " 1   id              2606 non-null   int64  \n",
      " 2   rawContent      2606 non-null   object \n",
      " 3   retweetedTweet  0 non-null      float64\n",
      " 4   quotedTweet     88 non-null     object \n",
      " 5   hashtags        0 non-null      float64\n",
      " 6   replyCount      2606 non-null   int64  \n",
      " 7   retweetCount    2606 non-null   int64  \n",
      " 8   likeCount       2606 non-null   int64  \n",
      " 9   quoteCount      2606 non-null   int64  \n",
      " 10  viewCount       2606 non-null   int64  \n",
      " 11  clean           2606 non-null   object \n",
      " 12  sentiment       2606 non-null   object \n",
      " 13  topic           2606 non-null   object \n",
      " 14  names           365 non-null    object \n",
      "dtypes: float64(2), int64(6), object(7)\n",
      "memory usage: 305.5+ KB\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "df = pd.read_csv('raw_tweet_data/elonmusk-tweets-with-topics-sentiment-name-2023-04-15.csv')\n",
    "df.info()\n",
    "selected_columns = ['rawContent','topic']\n",
    "ndf = df.loc[:,selected_columns].copy()\n",
    "ndf.head()\n",
    "ndf.to_csv('train/train_con_top_emusk.csv',index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93e6db0f-72eb-481c-ab63-22fa55724089",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
